# -*- coding: utf-8 -*-
"""DistributedTask2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vgBFfEGW4qt-H_1M76LH3IPg36e9Y0-m
"""

import threading
import random
from collections import Counter
import pandas as pd

shared_dataset = []

# binary lock used for synchronization
lock = threading.Lock()

#(1) entries
years = [2022, 2023, 2024]
courses = ["Machine Learning", "Cyber Security"]
universities = ["Stanford University", "MIT"]

# Function to add entries safely with lock
def add_entries(thread_id, num_entries):
    for _ in range(num_entries):
        entry = {
            "Thread": thread_id,
            "Year": random.choice(years),
            "Course Name": random.choice(courses),
            "Grade": random.randint(60, 100),
            "University": random.choice(universities)
        }

        # (2) lock used to ensure thread-safe access (simultaneous)
        with lock:
            shared_dataset.append(entry)

# (3a) function for simulating the concurrent data insertion
def simulate_concurrent(num_threads=10, entries_per_thread=5):
    threads = []
    # our list 'threads[]'
    for thread_id in range(num_threads):
       #from 0 to -1
        thread = threading.Thread(target=add_entries, args=(thread_id, entries_per_thread))
         #each thread will run
        #add entries, pass thread id and entries to add entries function
        threads.append(thread)
        #add new thread created to the list 'threads[]'
        thread.start()
         #concurrent execution of add entries

    for thread in threads:
        thread.join()
    # after starting all threads we now wait for each of them to finish
    # makes sure the main program doesn't continue until all threads are done
    # "Join": wait here until this specific thread finishes

# (3b) function for checking dataset consistency (final)
def verify_dataset(expected_total, output_file="output.txt"):
    actual_total = len(shared_dataset)

    # writes the output in text file
    output_lines = []
    output_lines.append(f"\nTotal entries in dataset: {actual_total}")
    if actual_total != expected_total:
        output_lines.append(f"Inconsistency detected! Missing entries: {expected_total - actual_total}")
    else:
        output_lines.append("Dataset is consistent")

    df = pd.DataFrame(shared_dataset)
    output_lines.append("\nDistribution per Course:")
    output_lines.append(df['Course Name'].value_counts().to_string())


    with open(output_file, 'w') as f:
        f.write('\n'.join(output_lines))


    print('\n'.join(output_lines))

# (4) function adds entries to the shared dataset WITHOUT using a lock.
# we made it to demonstrate what happens when threads access shared data unsafely.
def add_entries_no_lock(thread_id, num_entries):
    for _ in range(num_entries):
        # create a new entry with random values
        entry = {
            "Thread": thread_id,
            "Year": random.choice(years),  # random select a year
            "Course Name": random.choice(courses),  # random course
            "Grade": random.randint(60, 100),  # random grade between 60-100
            "University": random.choice(universities)  # random university
        }

        # directly add the entry to the shared dataset
        # no lock is used here, so multiple threads might interfere with each other
        # this can lead to missing or corrupted entries due to race conditions
        shared_dataset.append(entry)

# function simulates multiple threads adding entries WITHOUT using a lock
# meant to show how skipping synchronization can lead to data loss or inconsistency
def simulate_no_lock(num_threads=10, entries_per_thread=5):
    threads = []

    for thread_id in range(num_threads):
        # each thread will run add_entries_no_lock and try to add entries at the same time
        thread = threading.Thread(target=add_entries_no_lock, args=(thread_id, entries_per_thread))
        threads.append(thread)
        thread.start()  # start thread

    for thread in threads:
        thread.join()  # wait for all threads to finish before moving on

# Test with lock
print("Testing WITH lock:")
shared_dataset = []
simulate_concurrent(num_threads=10, entries_per_thread=5)
verify_dataset(expected_total=10*5, output_file="with_lock_results.txt")

# Test without lock
print("\nTesting WITHOUT lock:")
shared_dataset = []
simulate_no_lock(num_threads=10, entries_per_thread=5)
verify_dataset(expected_total=10*5, output_file="no_lock_results.txt")